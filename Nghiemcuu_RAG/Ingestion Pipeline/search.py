# 29/06 bá»• sung thÃªm promt
# Káº¿t ná»‘t llm cháº£ lá»i chÃ­nh xÃ¡c hÆ¡n

import os
import yaml
import json
import requests
from typing import Optional, List
from langchain.llms.base import LLM
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS

# === 1. Load cáº¥u hÃ¬nh ===
with open("config.yaml", "r", encoding="utf-8") as f:
    config = yaml.safe_load(f)

embedding = HuggingFaceEmbeddings(model_name=config['embedding_model'])

db_path = config["vectorstore_path"]
if not os.path.exists(db_path):
    raise FileNotFoundError("âŒ Vectorstore chÆ°a Ä‘Æ°á»£c táº¡o. Vui lÃ²ng cháº¡y chunk_and_ingest.py.")

# === 2. Load FAISS retriever ===
db = FAISS.load_local(db_path, embedding, allow_dangerous_deserialization=True)
retriever = db.as_retriever(search_kwargs={"k": 5})

# === 3. Lá»›p káº¿t ná»‘i LM Studio LLM ===
class LMStudioLLM(LLM):
    model: str = "llama2-7b-chat"  # hoáº·c thay báº±ng tÃªn model báº¡n Ä‘ang dÃ¹ng
    base_url: str = "http://localhost:1234/v1/chat/completions"
    temperature: float = 0.7
    max_tokens: int = 512

    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:
        messages = [{"role": "user", "content": prompt}]
        payload = {
            "model": self.model,
            "messages": messages,
            "temperature": self.temperature,
            "max_tokens": self.max_tokens,
        }
        headers = {"Content-Type": "application/json"}

        response = requests.post(self.base_url, headers=headers, data=json.dumps(payload))
        if response.status_code == 200:
            return response.json()["choices"][0]["message"]["content"]
        else:
            raise Exception(f"Lá»—i gá»i LM Studio: {response.status_code} - {response.text}")

    @property
    def _llm_type(self) -> str:
        return "lm_studio"

# === 4. Khá»Ÿi táº¡o LLM ===
llm = LMStudioLLM()

# === 5. PromptTemplate tráº£ lá»i tá»± nhiÃªn ===
prompt_template = PromptTemplate(
    input_variables=["context", "question"],
    template="""
Báº¡n lÃ  má»™t trá»£ lÃ½ AI thÃ¢n thiá»‡n, chuyÃªn há»— trá»£ ngÆ°á»i dÃ¹ng tráº£ lá»i cÃ¡c cÃ¢u há»i dá»±a trÃªn tÃ i liá»‡u sau.

HÃ£y Ä‘á»c ká»¹ pháº§n tÃ i liá»‡u vÃ  tráº£ lá»i báº±ng **tiáº¿ng Viá»‡t rÃµ rÃ ng, tá»± nhiÃªn, Ä‘Ãºng trá»ng tÃ¢m**.

### TÃ i liá»‡u:
{context}

### CÃ¢u há»i:
{question}

### Tráº£ lá»i:
"""
)

# === 6. Táº¡o RetrievalQA chain dÃ¹ng prompt tÃ¹y chá»‰nh ===
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    return_source_documents=True,
    chain_type_kwargs={"prompt": prompt_template}
)

# === 7. VÃ²ng láº·p CLI há»i Ä‘Ã¡p ===
while True:
    query = input("\nğŸ” Nháº­p cÃ¢u há»i (hoáº·c 'exit' Ä‘á»ƒ thoÃ¡t): ").strip()
    if query.lower() == "exit":
        print("ğŸ‘‹ ThoÃ¡t.")
        break

    result = qa_chain.invoke(query)

    print("\nğŸ¤– CÃ¢u tráº£ lá»i:")
    print(result["result"].strip())

    print("\nğŸ“‚ Nguá»“n trÃ­ch:")
    for doc in result["source_documents"]:
        print(f"- {doc.metadata.get('source', 'KhÃ´ng rÃµ')}")

import os
import yaml
import json
import re
import unicodedata
import requests
from typing import Optional, List
from langchain.llms.base import LLM
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQA

# === HÃ m chuáº©n hÃ³a cÃ¢u há»i ===
def clean_query(text: str) -> str:
    text = unicodedata.normalize("NFKC", text)
    text = text.lower().strip()
    text = text.replace("\n", " ")
    text = re.sub(r"\s+", " ", text)
    return text

# === Load config.yaml ===
with open("config.yaml", "r", encoding="utf-8") as f:
    config = yaml.safe_load(f)

embedding = HuggingFaceEmbeddings(model_name=config["embedding_model"])
db_path = config["vectorstore_path"]
if not os.path.exists(db_path):
    raise FileNotFoundError("âŒ Vectorstore chÆ°a Ä‘Æ°á»£c táº¡o. Vui lÃ²ng cháº¡y chunk_and_ingest.py.")

db = FAISS.load_local(db_path, embedding, allow_dangerous_deserialization=True)
retriever = db.as_retriever(search_kwargs={"k": 5})

# === Lá»›p káº¿t ná»‘i LM Studio ===
class LMStudioLLM(LLM):
    model: str = "llama2-7b-chat"
    base_url: str = "http://localhost:1234/v1/chat/completions"
    temperature: float = 0.7
    max_tokens: int = 700

    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:
        messages = [{"role": "user", "content": prompt}]
        payload = {
            "model": self.model,
            "messages": messages,
            "temperature": self.temperature,
            "max_tokens": self.max_tokens,
        }
        headers = {"Content-Type": "application/json"}
        response = requests.post(self.base_url, headers=headers, data=json.dumps(payload))
        if response.status_code == 200:
            return response.json()["choices"][0]["message"]["content"]
        else:
            raise Exception(f"Lá»—i gá»i LM Studio: {response.status_code} - {response.text}")

    @property
    def _llm_type(self) -> str:
        return "lm_studio"

# === Khá»Ÿi táº¡o mÃ´ hÃ¬nh ===
llm = LMStudioLLM()
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    return_source_documents=True
)

# === CLI Loop: há»i â†’ refine tráº£ lá»i ===
while True:
    query = input("\nğŸ” Nháº­p cÃ¢u há»i (hoáº·c 'exit' Ä‘á»ƒ thoÃ¡t): ").strip()
    if query.lower() == "exit":
        print("ğŸ‘‹ ThoÃ¡t.")
        break

    cleaned_query = clean_query(query)

    # --- VÃ²ng 1: RAG tráº£ lá»i tá»« context ---
    result1 = qa_chain.invoke(cleaned_query)
    first_answer = result1["result"].strip()

    # --- Láº¥y context gá»‘c tá»« vectorstore ---
    retrieved_context = "\n\n".join([doc.page_content for doc in result1["source_documents"]])

    # --- VÃ²ng 2: LLM refine láº¡i cÃ¢u tráº£ lá»i ---
    refine_prompt = f"""
Báº¡n lÃ  má»™t trá»£ lÃ½ AI. DÆ°á»›i Ä‘Ã¢y lÃ  má»™t sá»‘ tÃ i liá»‡u liÃªn quan vÃ  cÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng. HÃ£y Ä‘á»c ká»¹ tÃ i liá»‡u vÃ  Ä‘Æ°a ra cÃ¢u tráº£ lá»i chÃ­nh xÃ¡c, rÃµ rÃ ng vÃ  Ä‘áº§y Ä‘á»§ nháº¥t **báº±ng tiáº¿ng Viá»‡t tá»± nhiÃªn**.

### TÃ i liá»‡u:
{retrieved_context}

### CÃ¢u há»i:
{query}

### CÃ¢u tráº£ lá»i:
"""

    final_answer = llm._call(refine_prompt)

    # --- In káº¿t quáº£ ---
    print("\nğŸ¤– CÃ¢u tráº£ lá»i cuá»‘i cÃ¹ng:")
    print(final_answer.strip())

    print("\nğŸ“‚ Nguá»“n trÃ­ch:")
    for doc in result1["source_documents"]:
        print(f"- {doc.metadata.get('source', 'KhÃ´ng rÃµ')}")

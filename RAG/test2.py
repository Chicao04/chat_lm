import os
import requests
import json
from langchain_community.document_loaders import DirectoryLoader, TextLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.chains import ConversationalRetrievalChain
from langchain.schema import HumanMessage, AIMessage
from langchain.llms.base import LLM
from typing import Optional, List
from functools import partial
from langchain_huggingface import HuggingFaceEmbeddings


### ğŸ§© BÆ°á»›c 1: Táº¡o wrapper káº¿t ná»‘i LM Studio (OpenAI API format)

class LMStudioLLM(LLM):
    model: str = "llama2-7b-chat"
    base_url: str = "http://localhost:1234/v1/chat/completions"
    temperature: float = 0.7
    max_tokens: int = 100

    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:
        messages = [{"role": "user", "content": prompt}]
        payload = {
            "model": self.model,
            "messages": messages,
            "temperature": self.temperature,
            "max_tokens": self.max_tokens,
        }
        headers = {"Content-Type": "application/json"}

        response = requests.post(self.base_url, headers=headers, data=json.dumps(payload))
        if response.status_code == 200:
            return response.json()["choices"][0]["message"]["content"]
        else:
            raise Exception(f"Lá»—i gá»i LM Studio: {response.status_code} - {response.text}")

    @property
    def _llm_type(self) -> str:
        return "lm_studio"


### âœ… BÆ°á»›c 2: DÃ¹ng HuggingFace Embedding + FAISS : Chuyá»ƒn tá»‡p dá»¯ liá»‡u Ä‘áº§u vÃ o thÃ nh cÃ¡c vector

embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

index_path = "vector_index"
rebuild = False

# 2. Kiá»ƒm tra xem index Ä‘Ã£ tá»“n táº¡i chÆ°a
if os.path.exists(index_path):
    print("ğŸ“¦ Äang táº£i FAISS index tá»« á»• Ä‘Ä©a ...")
    vectorstore = FAISS.load_local(index_path, embedding_model)

    # 3. Load dá»¯ liá»‡u má»›i (tá»« thÆ° má»¥c ./data)
    loader = DirectoryLoader('./data', glob="**/*.txt", loader_cls=partial(TextLoader, encoding='utf-8'))
    documents = loader.load()

    # 4. Cáº¯t vÄƒn báº£n
    splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=50)
    texts = splitter.split_documents(documents)

    # 5. TÃ¹y chá»n: kiá»ƒm tra trÃ¹ng vÄƒn báº£n Ä‘á»ƒ trÃ¡nh thÃªm láº¡i
    print("ğŸ†• Äang cáº­p nháº­t FAISS vá»›i tÃ i liá»‡u má»›i ...")
    vectorstore.add_documents(texts)  # thÃªm vÃ o index hiá»‡n táº¡i
    vectorstore.save_local(index_path)

else:
    print("ğŸ“‚ Äang táº¡o FAISS index láº§n Ä‘áº§u ...")
    loader = DirectoryLoader('./data', glob="**/*.txt", loader_cls=partial(TextLoader, encoding='utf-8'))
    documents = loader.load()
    splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=50)
    texts = splitter.split_documents(documents)

    vectorstore = FAISS.from_documents(texts, embedding_model)
    vectorstore.save_local(index_path)

### âœ… BÆ°á»›c 4: XÃ¢y dá»±ng conversational RAG chain

llm = LMStudioLLM()
qa_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=vectorstore.as_retriever())
chat_history = []

### âœ… BÆ°á»›c 5: Giao diá»‡n chat Ä‘Æ¡n giáº£n

print("ğŸ’¬ Chatbot RAG Ä‘Ã£ sáºµn sÃ ng. GÃµ 'exit' Ä‘á»ƒ thoÃ¡t.\n")
while True:
    question = input("ğŸ‘¤ Báº¡n: ")
    if question.lower() in ['exit', 'thoÃ¡t']:
        print("ğŸ‘‹ Táº¡m biá»‡t!")
        break

    result = qa_chain.invoke({"question": question, "chat_history": chat_history})
    print("ğŸ¤– Bot:", result["answer"])
    chat_history.append((question, result["answer"]))
